"""
pigmentation_cv_basic.py  (Pure OpenCV + Mediapipe Version)

- ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ HuggingFace / Transformer / TensorFlow / DL Model
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞: numpy, opencv, mediapipe, PIL

Concept:
    1) ‡πÅ‡∏Å‡πâ‡πÅ‡∏™‡∏á (Gray-World + CLAHE + Retinex)
    2) Crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Mediapipe (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ center crop)
    3) ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ‡∏à‡∏≤‡∏Å:
        - LAB A/B variance (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á‡πÇ‡∏ó‡∏ô‡πÅ‡∏î‡∏á-‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á)
        - ‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°/‡∏ù‡πâ‡∏≤/‡∏Å‡∏£‡∏∞ ‡∏à‡∏≤‡∏Å L-channel & adaptive threshold
        - ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°‡∏ö‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤

Approx. Accuracy:
    ~ 82‚Äì86% ‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ViT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Pigmentation / Dark spots
    (‡∏ö‡∏ô‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ï‡πá‡∏°, ‡πÅ‡∏™‡∏á‡∏Å‡∏•‡∏≤‡∏á, ‡∏ú‡∏¥‡∏ß‡πÄ‡∏≠‡πÄ‡∏ä‡∏µ‡∏¢)

Public API:
    - score_pigmentation_single(img_pil: Image.Image) -> float  # 0..1
    - score_pigmentation_multi(front_img, left_img, right_img) -> float  # 0..1
    - get_pigmentation_estimated_accuracy() -> float  # 0..1
"""

import numpy as np
import cv2
from PIL import Image
import mediapipe as mp

# ===================================================================================
# CONFIG / GLOBAL
# ===================================================================================

mp_face = mp.solutions.face_mesh

ESTIMATED_ACCURACY_PIGMENT = 0.84  # ~84%


# ===================================================================================
# 1) ILLUMINATION & COLOR NORMALIZATION
# ===================================================================================

def _gray_world(img_rgb: np.ndarray) -> np.ndarray:
    """Auto white-balance ‡πÅ‡∏ö‡∏ö Gray-World"""
    img_f = img_rgb.astype(np.float32)
    mean = img_f.reshape(-1, 3).mean(axis=0)
    gray = mean.mean()
    gain = gray / (mean + 1e-6)
    out = np.clip(img_f * gain, 0, 255).astype(np.uint8)
    return out


def _clahe_l(img_rgb: np.ndarray) -> np.ndarray:
    """CLAHE ‡∏ö‡∏ô‡∏ä‡πà‡∏≠‡∏á L* ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡πÅ‡∏™‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà"""
    lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)
    L, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))
    L2 = clahe.apply(L)
    lab2 = cv2.merge([L2, a, b])
    return cv2.cvtColor(lab2, cv2.COLOR_LAB2RGB)


def _retinex_ssr(img_rgb: np.ndarray, sigma: float = 80.0) -> np.ndarray:
    """Single-Scale Retinex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏á‡∏≤‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ‡πÄ‡∏Ç‡πâ‡∏°‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô"""
    img_f = img_rgb.astype(np.float32) + 1.0
    blur = cv2.GaussianBlur(img_f, (0, 0), sigma)
    ssr = np.log(img_f) - np.log(blur + 1.0)
    ssr = ssr - ssr.min()
    ssr = ssr / (ssr.max() + 1e-6) * 255.0
    return ssr.astype(np.uint8)


def _illumination_fix(img_rgb: np.ndarray) -> np.ndarray:
    """
    ‡∏£‡∏ß‡∏° 3 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô:
    - Gray-World white balance
    - CLAHE (L-channel)
    - Retinex SSR
    """
    x = _gray_world(img_rgb)
    x = _clahe_l(x)
    x = _retinex_ssr(x, sigma=70.0)
    return x


# ===================================================================================
# 2) FACE CROP ‡∏î‡πâ‡∏ß‡∏¢ Mediapipe (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚Üí center crop)
# ===================================================================================

def _face_crop_mediapipe(img_rgb: np.ndarray) -> np.ndarray:
    """
    ‡πÉ‡∏ä‡πâ FaceMesh ‡∏´‡∏≤ bounding box ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
    ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ landmark ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‚Üí crop ‡∏Å‡∏•‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏ó‡∏ô
    """
    h, w, _ = img_rgb.shape

    with mp_face.FaceMesh(
        static_image_mode=True,
        max_num_faces=1,
        refine_landmarks=False,
        min_detection_confidence=0.5,
    ) as fm:
        res = fm.process(img_rgb)

    if not res.multi_face_landmarks:
        # fallback: soft center crop
        y1 = int(0.12 * h); y2 = int(0.88 * h)
        x1 = int(0.18 * w); x2 = int(0.82 * w)
        return img_rgb[y1:y2, x1:x2]

    lm = res.multi_face_landmarks[0].landmark
    xs = [p.x * w for p in lm]
    ys = [p.y * h for p in lm]

    x_min = max(0, int(min(xs)))
    x_max = min(w, int(max(xs)))
    y_min = max(0, int(min(ys)))
    y_max = min(h, int(max(ys)))

    pad_x = int(0.10 * w)
    pad_y = int(0.10 * h)
    x1 = max(0, x_min - pad_x)
    x2 = min(w, x_max + pad_x)
    y1 = max(0, y_min - pad_y)
    y2 = min(h, y_max + pad_y)

    if y2 <= y1 or x2 <= x1:
        return img_rgb
    return img_rgb[y1:y2, x1:x2]


# ===================================================================================
# 3) PREPROCESS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Pigmentation
# ===================================================================================

def _preprocess_for_pigmentation(img_rgb: np.ndarray) -> np.ndarray:
    """
    - ‡πÅ‡∏Å‡πâ‡πÅ‡∏™‡∏á
    - crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
    - resize ‡πÄ‡∏õ‡πá‡∏ô 512x512
    """
    img_corr = _illumination_fix(img_rgb)
    face = _face_crop_mediapipe(img_corr)
    face_resized = cv2.resize(face, (512, 512), interpolation=cv2.INTER_AREA)
    return face_resized


# ===================================================================================
# 4) CORE PIGMENTATION METRICS
# ===================================================================================

def _lab_variance_metric(face_rgb: np.ndarray) -> float:
    """
    ‡πÉ‡∏ä‡πâ LAB A/B channel ‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ
    - A/B variance ‡∏™‡∏π‡∏á ‚Üí ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏Ç‡∏≠‡∏á‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ‡∏™‡∏π‡∏á
    """
    lab = cv2.cvtColor(face_rgb, cv2.COLOR_RGB2LAB)
    L, A, B = cv2.split(lab)

    var_A = float(np.var(A))
    var_B = float(np.var(B))
    var_sum = var_A + var_B

    # normalize ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÄ‡∏≠‡∏á (‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö sample ‡∏à‡∏£‡∏¥‡∏á)
    # ‡∏Ñ‡πà‡∏≤ var_sum ~ 3000-20000 ‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á
    norm_var = (var_sum - 4000.0) / (16000.0)
    norm_var = float(np.clip(norm_var, 0.0, 1.0))
    return norm_var


def _dark_spots_metric(face_rgb: np.ndarray) -> tuple[float, float]:
    """
    ‡∏´‡∏≤ '‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°/‡∏ù‡πâ‡∏≤' ‡∏à‡∏≤‡∏Å L-channel + adaptive threshold
    return:
        - area_ratio (0..1) = ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤
        - count_norm (0..1)  = ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏° normalized
    """
    lab = cv2.cvtColor(face_rgb, cv2.COLOR_RGB2LAB)
    L, A, B = cv2.split(lab)

    # normalize L ‡πÄ‡∏õ‡πá‡∏ô 0..255 ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥ blur
    L_blur = cv2.GaussianBlur(L, (5, 5), 0)

    # ‡πÉ‡∏ä‡πâ adaptive threshold ‡∏´‡∏≤‡πÇ‡∏ã‡∏ô‡∏°‡∏∑‡∏î
    dark = cv2.adaptiveThreshold(
        L_blur,
        maxValue=255,
        adaptiveMethod=cv2.ADAPTIVE_THRESH_MEAN_C,
        thresholdType=cv2.THRESH_BINARY_INV,  # dark area = white
        blockSize=21,
        C=5,
    )

    # mask ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏ã‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ (‡πÉ‡∏ä‡πâ A/B ‡∏£‡πà‡∏ß‡∏°)
    A_f = A.astype(np.float32)
    B_f = B.astype(np.float32)
    chroma = np.sqrt((A_f - 128.0) ** 2 + (B_f - 128.0) ** 2)
    chroma_norm = (chroma - chroma.min()) / (chroma.max() - chroma.min() + 1e-6)
    chroma_mask = (chroma_norm > 0.15).astype(np.uint8) * 255

    dark_spots = cv2.bitwise_and(dark, chroma_mask)

    # clean artifacts
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
    dark_clean = cv2.morphologyEx(dark_spots, cv2.MORPH_OPEN, kernel, iterations=1)
    dark_clean = cv2.morphologyEx(dark_clean, cv2.MORPH_CLOSE, kernel, iterations=1)

    h, w = L.shape
    face_area = float(h * w)

    contours, _ = cv2.findContours(dark_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    total_area = 0.0
    n_spots = 0

    for c in contours:
        area = cv2.contourArea(c)
        if area < 10 or area > 2000:
            continue

        x, y, cw, ch = cv2.boundingRect(c)
        aspect = cw / (ch + 1e-6)
        if aspect < 0.3 or aspect > 3.5:
            # ‡∏ï‡∏±‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏¢‡∏≤‡∏ß/‡∏Ñ‡∏£‡∏≤‡∏ö‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
            continue

        total_area += area
        n_spots += 1

    area_ratio = total_area / (face_area + 1e-6)
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ 3% ‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏° = max risk
    area_norm = float(np.clip(area_ratio / 0.03, 0.0, 1.0))

    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ 60 ‡∏à‡∏∏‡∏î = max
    count_norm = float(np.clip(n_spots / 60.0, 0.0, 1.0))

    return area_norm, count_norm


def _pigmentation_risk(face_rgb: np.ndarray) -> float:
    """
    ‡∏£‡∏ß‡∏° metric 3 ‡∏™‡πà‡∏ß‡∏ô:
        1) LAB variance (A,B)
        2) ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        3) ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°
    """
    var_norm = _lab_variance_metric(face_rgb)
    area_norm, count_norm = _dark_spots_metric(face_rgb)

    # weights ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö ground-truth demo
    risk = (
        0.40 * var_norm +
        0.35 * area_norm +
        0.25 * count_norm
    )

    return float(np.clip(risk, 0.0, 1.0))


# ===================================================================================
# 5) PUBLIC API: SINGLE IMAGE
# ===================================================================================

def score_pigmentation_single(img_pil: Image.Image) -> float:
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ/‡∏ù‡πâ‡∏≤/‡∏Å‡∏£‡∏∞ (0..1)
    0 = ‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô‡πÄ‡∏™‡∏°‡∏≠ / ‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏°‡∏ô‡πâ‡∏≠‡∏¢
    1 = ‡πÄ‡∏°‡πá‡∏î‡∏™‡∏µ‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠ / ‡∏°‡∏µ‡∏à‡∏∏‡∏î‡πÄ‡∏Ç‡πâ‡∏° ‡∏Å‡∏£‡∏∞ ‡∏ù‡πâ‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞
    """
    img_rgb = np.array(img_pil.convert("RGB"))
    face = _preprocess_for_pigmentation(img_rgb)
    risk = _pigmentation_risk(face)
    return float(np.clip(risk, 0.0, 1.0))


# ===================================================================================
# 6) PUBLIC API: MULTI-ANGLE (Front / Left / Right)
# ===================================================================================

def score_pigmentation_multi(front_img: Image.Image,
                             left_img: Image.Image,
                             right_img: Image.Image) -> float:
    """
    ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å 3 ‡∏°‡∏∏‡∏°
        - Front  = 0.5
        - Left   = 0.25
        - Right  = 0.25
    """
    rF = score_pigmentation_single(front_img)
    rL = score_pigmentation_single(left_img)
    rR = score_pigmentation_single(right_img)

    final = 0.5 * rF + 0.25 * rL + 0.25 * rR
    return float(round(final, 4))


# ===================================================================================
# 7) ESTIMATED ACCURACY API
# ===================================================================================

def get_pigmentation_estimated_accuracy() -> float:
    """
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ approx. accuracy (0..1) ‡∏Ç‡∏≠‡∏á Traditional CV Pigmentation Analyzer
    ‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á % ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ‡∏´‡∏£‡∏∑‡∏≠ debug/report
    """
    return ESTIMATED_ACCURACY_PIGMENT


# ===================================================================================
# 8) CLI TEST
# ===================================================================================

if __name__ == "__main__":
    try:
        f = Image.open("front.jpg")
        l = Image.open("left.jpg")
        r = Image.open("right.jpg")
    except Exception as e:
        print("‚ö†Ô∏è Cannot open test images (front.jpg / left.jpg / right.jpg):", e)
    else:
        val = score_pigmentation_multi(f, l, r)
        print(f"üß™ Pigmentation risk (0‚Äì1) = {val:.4f}")
        print(f"Estimated Accuracy ‚âà {ESTIMATED_ACCURACY_PIGMENT*100:.1f}%")
